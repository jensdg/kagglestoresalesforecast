{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About the forecast\n\nReading several of the other analyses helped me a lot to understand the data and also to start programming. I tried to implement the methods from the [Time Series Course](https://www.kaggle.com/learn/time-series). So the forecast combines linear regression with XGBoost. Then follows a multistep regression to forecast with the residuals of the boosted hybrid, again using XGBoost.\n\nThe main components of many time series are trend, seasonality and serial dependence ([machinelearningmastery.com](https://machinelearningmastery.com/time-series-forecasting/)). The linear regression in the boosted hybrid accounting for the trend and the seasonality extrapolates what XGBoost cannot ([towardsdatascience.com](https://towardsdatascience.com/xgboost-for-time-series-youre-gonna-need-a-bigger-boat-9d329efa6814)). Then the XGBoost Regressor uses the available predictors. The multistep regression following the boosted hybrid accounts for the serial dependence in the time series. ","metadata":{}},{"cell_type":"code","source":"# the first cell contains the imports and some required funktions\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.multioutput import RegressorChain\n\n\n# required functions - taken from the time series course and modified\n# for plots:\n\ndef lagplot(x, y=None, lag=1, standardize=False, ax=None, **kwargs):\n    from matplotlib.offsetbox import AnchoredText\n    x_ = x.shift(lag)\n    if standardize:\n        x_ = (x_ - x_.mean()) / x_.std()\n    if y is not None:\n        y_ = (y - y.mean()) / y.std() if standardize else y\n    else:\n        y_ = x\n    corr = y_.corr(x_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    scatter_kws = dict(\n        alpha=0.75,\n        s=3,\n    )\n    line_kws = dict(color='C3', )\n    ax = sns.regplot(x=x_,\n                     y=y_,\n                     scatter_kws=scatter_kws,\n                     line_kws=line_kws,\n                     lowess=True,\n                     ax=ax,\n                     **kwargs)\n    at = AnchoredText(\n        f\"{corr:.2f}\",\n        prop=dict(size=\"large\"),\n        frameon=True,\n        loc=\"upper left\",\n    )\n    at.patch.set_boxstyle(\"square, pad=0.0\")\n    ax.add_artist(at)\n    if lag >= 0:\n        ax.set(title=f\"Lag {lag}\", xlabel=x_.name, ylabel=y_.name)\n    if lag < 0:\n        ax.set(title=f\"Lead {-lag}\", xlabel=x_.name, ylabel=y_.name)\n    return ax\n\n\ndef plot_lags_leads(x, y=None, lags=6, leads=0, nrows=1, lagplot_kwargs={}, **kwargs):\n    import math\n    lag_0 = 0\n    if y is not None:\n        lag_0 = 1 # include lag_0 if correlation instead of autocorrelation \n    kwargs.setdefault('nrows', nrows)\n    kwargs.setdefault('ncols', math.ceil((lags + leads + lag_0) / nrows))\n    kwargs.setdefault('figsize', (kwargs['ncols'] * 2, nrows * 2 + 0.5))\n    nplots = kwargs['nrows'] * kwargs['ncols']\n    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)\n    for ax, k in zip(fig.get_axes(), range(nplots)):\n        if k + 1 <= leads:\n            ax = lagplot(x, y, lag = (k + 1) + (leads - nplots), ax=ax, **lagplot_kwargs)\n            ax.set_title(f\"Lead {-((k + 1) + (leads - nplots))}\", fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        elif (k + 1) > leads and (k + 1) - leads - lag_0 <= lags:\n            ax = lagplot(x, y, lag = (k + 1) - leads - lag_0, ax=ax, **lagplot_kwargs)\n            ax.set_title(f\"Lag {(k + 1) - leads - lag_0}\", fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        else:\n            ax.axis('off')\n    plt.setp(axs[-1, :], xlabel=x.name)\n    plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)\n    fig.tight_layout(w_pad=0.1, h_pad=0.1)\n    return fig\n\n\n# for time series:\n\ndef make_lags(ts, lags, lead_time=1):\n    return pd.concat(\n        {\n            f'{ts.name}_lag_{f\"{i:02}\"}': ts.shift(i)\n            for i in range(lead_time, lags + lead_time)\n        },\n        axis=1)\n\ndef make_leads(ts, leads):\n    return pd.concat(\n        {\n            f'{ts.name}_lead_{f\"{(i-1):02}\"}': ts.shift(-i+1)\n            for i in range(leads , 0, -1)\n        }, \n        axis=1)\n\ndef make_multistep_target(ts, steps):\n    return pd.concat(\n        {f'y_step_{f\"{(i+1):02}\"}': ts.shift(-i)\n         for i in range(steps)},\n        axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:19.857064Z","iopub.execute_input":"2022-03-30T07:40:19.85741Z","iopub.status.idle":"2022-03-30T07:40:21.569808Z","shell.execute_reply.started":"2022-03-30T07:40:19.857327Z","shell.execute_reply":"2022-03-30T07:40:21.569163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and processing the data sets","metadata":{}},{"cell_type":"code","source":"# read and format the data sets\n\ntrain = pd.read_csv(\n    '../input/store-sales-time-series-forecasting/train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n        'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\ntrain['date'] = train.date.dt.to_period('D')\ntrain = train.set_index(['store_nbr', 'family', 'date']).sort_index()\n\noil = pd.read_csv(\n    '../input/store-sales-time-series-forecasting/oil.csv',\n    dtype={\n        'dcoilwtico': 'float64',\n    },\n    parse_dates = ['date'], \n    infer_datetime_format = True,\n)    \noil = oil.set_index('date').to_period('D')\n\nholidays_events = pd.read_csv(\n    '../input/store-sales-time-series-forecasting/holidays_events.csv',\n    dtype={\n        'type': 'category',\n        'locale': 'category',\n        'locale_name': 'category',\n        'description': 'category',\n        'transferred': 'bool',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nholidays_events = holidays_events.set_index('date').to_period('D')\n\ntest = pd.read_csv(\n    '../input/store-sales-time-series-forecasting/test.csv',\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\ntest['date'] = test.date.dt.to_period('D')\ntest = test.set_index(['store_nbr', 'family', 'date']).sort_index()\n\n\n# treating missing dates in the data sets\n\n# there are missing dates in the training data set\ntrange = pd.DataFrame(index = pd.date_range('2013-01-01', '2017-08-15', name='date')).to_period('D') # actual training date range\ntrain = train.unstack(['store_nbr', 'family'])\ntrain = train.join(trange, how='right') # joins train with complete training range\n#train.isnull().sum().sum() # = 14256\n#dec25th = train[train.isnull().any(axis=1)] # find rows with NaN\ntrain = train.fillna(0) # fill with zeros as stores are closed on 25th of December\ntrain = train.stack(['store_nbr', 'family'])\n\n# oil data set also has missing dates\noilrange = pd.DataFrame(index = pd.date_range('2013-01-01', '2017-08-31', name='date')).to_period('D') # complete date range for oil\noil = oilrange.join(oil, how='left')\n#oil.isnull().sum().sum() # = 529\noil['dcoilwtico'] = oil['dcoilwtico'].interpolate(limit_direction='both') # fill NaNs with values in between\noil = oil.squeeze()\n\ndisplay(train)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:21.571222Z","iopub.execute_input":"2022-03-30T07:40:21.571856Z","iopub.status.idle":"2022-03-30T07:40:29.58714Z","shell.execute_reply.started":"2022-03-30T07:40:21.571819Z","shell.execute_reply":"2022-03-30T07:40:29.586194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features for the linear regression\n\nThe linear regression extrapolates the trend and seasonality in the data. From the perspective that time series consist mainly of level, trend, saisonality and noise it will explain a major part of the data. \n\nThe observable trend, the monthly seasonality and the level are modeled by the deterministic process. Calendar data is also used by the linear regression. It includes the data for the weekly seasonality and the calendar events.","metadata":{}},{"cell_type":"markdown","source":"**Deterministic process**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdate = '2015-01-15' # Start and end of training date range\nedate = '2017-08-15'\n\n# sales from training data set as target y\ny = train.unstack(['store_nbr', 'family']).drop('onpromotion', axis=1).droplevel(0, axis=1).loc[sdate:edate]\n\n# the deterministic process\nfourier = CalendarFourier(freq = 'M', order = 4)\ndp = DeterministicProcess(\n    index=y.index,\n    constant=True,\n    order=3,\n    seasonal=False,\n    additional_terms=[fourier],\n    drop=True,\n)\nX1 = dp.in_sample() # features from deterministic process for training\n\nX1_test = dp.out_of_sample(steps=16) # features for test\nX1_test.index.name = 'date'","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:29.590275Z","iopub.execute_input":"2022-03-30T07:40:29.590525Z","iopub.status.idle":"2022-03-30T07:40:30.382367Z","shell.execute_reply.started":"2022-03-30T07:40:29.590496Z","shell.execute_reply":"2022-03-30T07:40:30.381183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calendar features**\n\nAs the calendar events influence the sales they will be included as features.","metadata":{}},{"cell_type":"code","source":"# the date range of the calendar\ncalendar = pd.DataFrame(index = pd.date_range('2013-01-01', '2017-08-31', name='date')).to_period('D')\n\n# select the national and regional holidays events\nholidays = (\n    holidays_events\n    .query(\"locale in ['National', 'Regional']\")\n    .loc['2013-01-01':'2017-08-31', ['description']]\n    .assign(description=lambda x: x.description.cat.remove_unused_categories())\n)\n\n# deleting selected enrtries when dates have more than one\nholidays = holidays.loc[(holidays['description'] != 'Provincializacion de Imbabura') | ((holidays.index != '2014-06-25'))]\nholidays = holidays.loc[(holidays['description'] != 'Puente Navidad') | ((holidays.index != '2014-12-26'))]\nholidays = holidays.loc[(holidays['description'] != 'Dia del Trabajo') | ((holidays.index != '2016-05-01'))]\nholidays = holidays.loc[(holidays['description'] != 'Dia de la Madre-1') | ((holidays.index != '2016-05-07'))]\nholidays = holidays.loc[(holidays['description'] != 'Dia de la Madre') | ((holidays.index != '2016-05-08'))]\nholidays = holidays.loc[(holidays['description'] != 'Terremoto Manabi+15')]\nholidays = holidays.loc[(holidays['description'] != 'Terremoto Manabi+21')]\nholidays = holidays.loc[(holidays['description'] != 'Terremoto Manabi+22')]\n\n# add holidays events to the calendar\ncalendar = calendar.join(holidays)\n\n# encode the labels\ncalendar = pd.get_dummies(calendar, columns = ['description'])\n\n# to account for the weekly seasonality\ncalendar['dayofweek'] = calendar.index.dayofweek\ncalendar = pd.get_dummies(calendar, columns = ['dayofweek'], drop_first = True) # encode the labels\n\n# the days when salaries are paid out\ncalendar['15thOfMonth'] = (calendar.index.day == 15).astype(int)\ncalendar['lastOfMonth'] = (calendar.index.dayofyear == calendar.index.days_in_month).astype(int)\n\n# join deterministic process with calendar\nX1 = X1.join(calendar, on='date') \nX1_test = X1_test.join(calendar, on='date')\n\ndisplay(X1)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:30.387601Z","iopub.execute_input":"2022-03-30T07:40:30.388011Z","iopub.status.idle":"2022-03-30T07:40:30.491644Z","shell.execute_reply.started":"2022-03-30T07:40:30.387962Z","shell.execute_reply":"2022-03-30T07:40:30.491031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features for XGBoost\n\nAvailable data for the dates to be predicted in test are oil prices, promotions and lagged values of the target y.","metadata":{}},{"cell_type":"code","source":"# examine the relationships on the residuals of the linear regression\nlnr = LinearRegression(fit_intercept=False)\nlnr.fit(X1, y)\ny_fit = pd.DataFrame(lnr.predict(X1), index=X1.index, columns=y.columns).clip(0.0)\ny_deseasoned = y - y_fit\n# grouped by family\ny_des_family = ( \n    y_deseasoned\n    .stack(['store_nbr','family'])\n    .groupby(['family', 'date']).mean()\n    .unstack('family')\n    .squeeze()\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:30.493238Z","iopub.execute_input":"2022-03-30T07:40:30.493555Z","iopub.status.idle":"2022-03-30T07:40:30.910682Z","shell.execute_reply.started":"2022-03-30T07:40:30.493513Z","shell.execute_reply":"2022-03-30T07:40:30.909916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Oil prices**","metadata":{}},{"cell_type":"code","source":"ax = oil.plot(title = 'Oil prices', ylabel='dcoilwtico')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:30.912073Z","iopub.execute_input":"2022-03-30T07:40:30.912474Z","iopub.status.idle":"2022-03-30T07:40:31.218704Z","shell.execute_reply.started":"2022-03-30T07:40:30.912395Z","shell.execute_reply":"2022-03-30T07:40:31.217699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The influence of the oil prices on the detrended target value is very low. To avoid lookahead leakage only actual and lagged values can be used.","metadata":{}},{"cell_type":"code","source":"# show the influence of oil prices on sales\nofamily = 'GROCERY I' #'SCHOOL AND OFFICE SUPPLIES'\nplot_lags_leads(x=oil.loc[sdate:edate],\n                y=y_des_family.loc(axis=1)[ofamily].rename(ofamily), \n                lags=2, nrows=1);","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:31.220426Z","iopub.execute_input":"2022-03-30T07:40:31.220747Z","iopub.status.idle":"2022-03-30T07:40:32.052879Z","shell.execute_reply.started":"2022-03-30T07:40:31.220702Z","shell.execute_reply":"2022-03-30T07:40:32.051935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even if over a longer time horizon, with the data from before 2015, the oil prices show a negative correlation with the non stationary sales. In the shorter run there is a low correlation or even positive correlation, which doesn't seem reasonable. It seems that price shocks can affect the sales by decreasing the incomes. But shifts over weeks or days may not have a an influence on the sales. Did not find a way yet to apply the data set.","metadata":{}},{"cell_type":"code","source":"odate='2013-01-01' #'2016-01-01'\n\n# group sales by family\nsales_family = ( \n    train\n    .unstack(['store_nbr', 'family'])\n    .drop('onpromotion', axis=1)\n    .droplevel(0, axis=1)\n    .stack(['store_nbr','family'])\n    .groupby(['family', 'date']).mean()\n    .unstack('family')\n    .squeeze()\n)\n\n# show relationship between oil prices and sales\nplot_lags_leads(x=oil.loc[odate:edate], y=sales_family.loc(axis=1)[ofamily].loc[odate:edate].rename(ofamily), lags=2, nrows=1);","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:32.054421Z","iopub.execute_input":"2022-03-30T07:40:32.054896Z","iopub.status.idle":"2022-03-30T07:40:34.433852Z","shell.execute_reply.started":"2022-03-30T07:40:32.054863Z","shell.execute_reply":"2022-03-30T07:40:34.433293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Promotions**\n\nThe onpromotions data correlates with the detrended sales. As the stores know about their promotions lagged as well as leading values could be used. Thus, the promotions data can be applied as a leading indicator. Both leading and lagged values seem to be useful as features.","metadata":{}},{"cell_type":"code","source":"# show relationship with family\nrfamily = 'SCHOOL AND OFFICE SUPPLIES'\n\n# onpromotion data from train\nonpromotion = train.unstack(['store_nbr', 'family']).drop('sales', axis=1).droplevel(0, axis=1).loc[sdate:edate]\nonpromotion.name = 'onpromotion'\n# group by family\nfamily_onpromotion = ( \n    onpromotion\n    .stack(['store_nbr','family'])\n    .groupby(['family', 'date']).mean()\n    .unstack('family')\n    .loc(axis=1)[rfamily]                                  \n    .squeeze()\n    .rename('family_onpromotion')\n)\nplot_lags_leads(x=family_onpromotion.loc[family_onpromotion > 1], \n                y=y_des_family.loc[family_onpromotion > 1].loc(axis=1)[rfamily].rename(rfamily), \n                lags=3, leads=3, nrows=1);","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:34.435294Z","iopub.execute_input":"2022-03-30T07:40:34.435508Z","iopub.status.idle":"2022-03-30T07:40:36.412904Z","shell.execute_reply.started":"2022-03-30T07:40:34.43548Z","shell.execute_reply":"2022-03-30T07:40:36.412203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lagged values**\n\nUsing the lagged values of the target y may account for the serial dependence in the time series. As sales values are available only for the training data set they could potentially be used to forecast just one date in test. Therefore a multistep regression will forecast the whole time horizon of test which also accounts for the serial dependence in y. It will be applied to the residuals resulting from the boosted hybrid.\n\nTaking a look at for example the school and office supplies time series the partial autocorrelations suggest that some lags up to the 9th could be used. The lagplots indicate a mostly linear relationship but a non-linear relationship might play a role as well. Finally the number of lags will be chosen for all the product families together.","metadata":{}},{"cell_type":"code","source":"rfamily = 'SCHOOL AND OFFICE SUPPLIES'\nplot_pacf(y_des_family.loc(axis=1)[rfamily].loc[sdate:edate], lags=12);\nplot_lags_leads(y_des_family.loc(axis=1)[rfamily].loc[sdate:edate], lags=10, nrows=2);","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:36.416043Z","iopub.execute_input":"2022-03-30T07:40:36.416463Z","iopub.status.idle":"2022-03-30T07:40:39.390782Z","shell.execute_reply.started":"2022-03-30T07:40:36.416427Z","shell.execute_reply":"2022-03-30T07:40:39.389879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building the features for XGBoost**","metadata":{}},{"cell_type":"code","source":"# add onpromotion data from test data set\nonpromotion_test = test.drop('id', axis=1).unstack(['store_nbr', 'family']).droplevel(0, axis=1)\nonpromotion = pd.concat([onpromotion, onpromotion_test]).sort_index()\nonpromotion.name = 'onpromotion'\n\n# add onpromotion time series as features\nX2 = pd.concat([\n    make_lags(onpromotion, lags=2),\n    pd.concat({'onpromotion_lag_0': onpromotion}, names=[None, 'store_nbr', 'family'], axis=1),\n    make_leads(onpromotion, leads=3)], \n    names=[None, 'store_nbr' ,'family'],\n    axis=1)\n\nX2 = X2.stack(['store_nbr', 'family'])\n\n# experimenting with oil data without success\n#oil_lags = make_lags(oil, lags=3)\n#X2 = X2.join(oil_lags)\nX2 = X2.fillna(0.0)\n\n# Label encoding store_nbr and family\nX2 = X2.reset_index(['store_nbr', 'family'])\n\nfor colname in X2.select_dtypes([\"object\", \"category\"]):\n    X2[colname], _ = X2[colname].factorize()\n\nX2_test = X2.loc[\"2017-08-16\" : \"2017-08-31\"] # features for the test period\nX2 = X2.loc[sdate : edate]\n\ndisplay(X2)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:39.391893Z","iopub.execute_input":"2022-03-30T07:40:39.392306Z","iopub.status.idle":"2022-03-30T07:40:41.683571Z","shell.execute_reply.started":"2022-03-30T07:40:39.39227Z","shell.execute_reply":"2022-03-30T07:40:41.683004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The boosted hybrid model","metadata":{}},{"cell_type":"code","source":"# this class combines linear regression or other model with XGBoost - from time series course and slightly modified\nclass BoostedHybrid:\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None  # store column names from fit method\n\ndef fit(self, X_1, X_2, y):\n    model = self.model_1\n    model.fit(X_1,y)\n\n    y_fit = pd.DataFrame(\n        model.predict(X_1),\n        index=X_1.index, columns=y.columns,\n    )\n\n    y_resid = y - y_fit\n  \n    y_resid = y_resid.stack(list(range(y_resid.columns.nlevels))).squeeze() # stack all levels\n    self.model_2.fit(X_2, y_resid)\n\n    # Save column names for predict method\n    self.y_columns = y.columns\n    # Save data for question checking\n    self.y_fit = y_fit\n    self.y_resid = y_resid\n\n\n# Add method to class\nBoostedHybrid.fit = fit\n\ndef predict(self, X_1, X_2):\n    y_pred = pd.DataFrame(\n        self.model_1.predict(X_1),\n        index=X_1.index, columns=self.y_columns,\n    )\n\n    y_pred = y_pred.stack(list(range(y_pred.columns.nlevels))).squeeze() # stack all levels\n\n    y_pred += self.model_2.predict(X_2)\n    \n    return y_pred.unstack(list(range(y_pred.index.nlevels)[1:])) # unstack all levels except the first\n\n\n# Add method to class\nBoostedHybrid.predict = predict\n\nbhybrid = BoostedHybrid(LinearRegression(fit_intercept=False), XGBRegressor(n_estimators=150, eta=0.12, max_depth=6))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:41.684605Z","iopub.execute_input":"2022-03-30T07:40:41.684928Z","iopub.status.idle":"2022-03-30T07:40:41.695933Z","shell.execute_reply.started":"2022-03-30T07:40:41.6849Z","shell.execute_reply":"2022-03-30T07:40:41.695162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features for the multistep regression\n\nThe multistep regression uses the residuals of the boosted hybrid. ","metadata":{}},{"cell_type":"code","source":"# calculating the residuals of the boosted hybrid prediction\nbhybrid.fit(X1, X2, y)\ny_fit = bhybrid.predict(X1, X2).clip(0.0)\ny_pred = bhybrid.predict(X1_test, X2_test).clip(0.0) # also needed for the submission\ny_resid = y - y_fit\ny_resid.name = 'sales_residuals'","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:40:41.697101Z","iopub.execute_input":"2022-03-30T07:40:41.697427Z","iopub.status.idle":"2022-03-30T07:42:32.394077Z","shell.execute_reply.started":"2022-03-30T07:40:41.697386Z","shell.execute_reply":"2022-03-30T07:42:32.39344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building the features for the multistep regression**","metadata":{}},{"cell_type":"code","source":"# the features for the multistep regression\n\n# using the lagged residuals as features for the forecast\nX3 = make_lags(y_resid, lags=8).dropna(axis=0)\nX3_test = X3.loc[['2017-08-15']] # features for the test forecast\n\n# generating the multistep target ym\nnsteps = 16 # number of multistep targets\nym = make_multistep_target(y_resid, steps=nsteps).dropna(axis=0)\nym, X3 = ym.align(X3, join='inner', axis=0)\nym = ym.stack(['store_nbr', 'family'])  # wide to long\n\n# encoding labels\nle = LabelEncoder()\nX3 = (X3\n    .stack(['store_nbr', 'family'])  # wide to long\n    .reset_index(['store_nbr','family'])  # convert index to column\n    .assign(store_nbr=lambda x: le.fit_transform(x.store_nbr))  # label encode store_nbr\n    .assign(family=lambda x: le.fit_transform(x.family))  # label encode family\n)\n\nX3_test = (X3_test\n    .stack(['store_nbr', 'family'])  # wide to long\n    .reset_index(['store_nbr','family'])  # convert index to column\n    .assign(store_nbr=lambda x: le.fit_transform(x.store_nbr))  # label encode store_nbr\n    .assign(family=lambda x: le.fit_transform(x.family))  # label encode family\n)\n\ndisplay(X3)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:42:32.39521Z","iopub.execute_input":"2022-03-30T07:42:32.395755Z","iopub.status.idle":"2022-03-30T07:42:38.011785Z","shell.execute_reply.started":"2022-03-30T07:42:32.395721Z","shell.execute_reply":"2022-03-30T07:42:38.010856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation of the boosted hybrid","metadata":{}},{"cell_type":"code","source":"# evaluating the boosted hybrid\n\ny_train, y_valid = y[: \"2017-07-30\"], y[\"2017-07-31\" :] # last 16 days for validation\nX1_train, X1_valid = X1[: \"2017-07-30\"], X1[\"2017-07-31\" :]\nX2_train, X2_valid = X2[: \"2017-07-30\"], X2[\"2017-07-31\" :]\n\nbhybrid.fit(X1_train, X2_train, y_train)\n\ny_fit_boosted = bhybrid.predict(X1_train, X2_train).clip(0.0)\ny_pred_boosted = bhybrid.predict(X1_valid, X2_valid).clip(0.0)\n\nrmsle_train_boosted = mean_squared_log_error(y_train, y_fit_boosted, squared=False)\nrmsle_valid_boosted = mean_squared_log_error(y_valid, y_pred_boosted, squared=False)\n\nprint(f'Training boosted hybrid RMSLE: {rmsle_train_boosted:.5f}')\nprint(f'Validation boosted hybrid RMSLE: {rmsle_valid_boosted:.5f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:42:38.013155Z","iopub.execute_input":"2022-03-30T07:42:38.013386Z","iopub.status.idle":"2022-03-30T07:44:17.789138Z","shell.execute_reply.started":"2022-03-30T07:42:38.013357Z","shell.execute_reply":"2022-03-30T07:44:17.788182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation of the multistep regression","metadata":{}},{"cell_type":"code","source":"# evaluating the multiple target step regression\n# this cell takes a while\n\nmtsdate = '2017-04-05' # start of training for multitarget regression\n\nym_train, ym_valid = ym.loc[mtsdate: '2017-07-15'], ym.loc['2017-07-30']\nX3_train, X3_valid = X3.loc[mtsdate: '2017-07-15'], X3.loc['2017-07-30']\n\nmtxgb = RegressorChain(XGBRegressor(n_estimators=150, eta=0.1, max_depth=12))\n\nmtxgb.fit(X3_train, ym_train)\n\nym_fore_boosted = pd.DataFrame(mtxgb.predict(X3_valid), index=ym_valid.index, columns=ym_valid.columns).clip(0.0)\n\n# reshape the row of forecasted values:\nym_fore_boosted = (\n    ym_fore_boosted\n    .stack()\n    .reset_index()\n    .drop(columns=['date']) # drop former date column\n    # replace 'Y_step_01' to 'y_step_16' by dates from '2017-07-31' to '2017-08-15':\n    .replace([f'y_step_{f\"{(i+1):02}\"}' for i in range(nsteps)], pd.date_range('2017-07-31', '2017-08-15').to_period('D'))\n    .rename(columns={'level_3' : 'date', 0 : 'sales'})\n    .set_index(['date','store_nbr', 'family'])\n    .sort_index()\n    .unstack(['store_nbr', 'family'])\n    .droplevel(0, axis=1)\n)\n\nrmsle_valid_mboosted = mean_squared_log_error(y_valid, (ym_fore_boosted + y_pred_boosted), squared=False)\nprint(f'Validation multitarget XGBR RMSLE: {rmsle_valid_mboosted:.5f}')\n\nrmsle_family_valid_mboosted = (ym_fore_boosted + y_pred_boosted).stack(['store_nbr', 'family']).rename('sales').to_frame()\nrmsle_family_valid_mboosted['true_sales'] = y_valid.stack(['store_nbr', 'family'])\nrmsle_family_valid_mboosted.groupby('family').apply(lambda r: mean_squared_log_error(r['sales'], r['true_sales'], squared=False))","metadata":{"execution":{"iopub.status.busy":"2022-03-30T07:44:17.79043Z","iopub.execute_input":"2022-03-30T07:44:17.790638Z","iopub.status.idle":"2022-03-30T08:13:27.991972Z","shell.execute_reply.started":"2022-03-30T07:44:17.790612Z","shell.execute_reply":"2022-03-30T08:13:27.991098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"mtxgb.fit(X3.loc[mtsdate:], ym.loc[mtsdate:])\nym_fore = pd.DataFrame(mtxgb.predict(X3_test), index=y.loc[['2017-08-15']].stack(['store_nbr', 'family']).index, columns=ym.columns).clip(0.0)\n\n# reshape the row of forecasted values:\nym_fore = (\n    ym_fore\n    .stack()\n    .reset_index()\n    .drop(columns=['date']) # drop former date column\n    # replace 'Y_step_01' to 'y_step_16' by dates from '2017-08-16' to '2017-08-31':\n    .replace([f'y_step_{f\"{(i+1):02}\"}' for i in range(nsteps)], pd.date_range('2017-08-16', '2017-08-31').to_period('D'))\n    .rename(columns={'level_3' : 'date', 0 : 'sales'})\n    .set_index(['date','store_nbr', 'family'])\n    .sort_index()\n    .unstack(['store_nbr', 'family'])\n    .droplevel(0, axis=1)\n)\n\ny_submit = y_pred + ym_fore\ny_submit = y_submit.stack(['store_nbr', 'family']).rename('sales').to_frame()\ny_submit = y_submit.join(test.id).reindex(columns=['id', 'sales'])\ny_submit.to_csv('submission.csv', index=False)\ndisplay(y_submit)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T08:13:27.995305Z","iopub.execute_input":"2022-03-30T08:13:27.995555Z","iopub.status.idle":"2022-03-30T08:47:28.258378Z","shell.execute_reply.started":"2022-03-30T08:13:27.995528Z","shell.execute_reply":"2022-03-30T08:47:28.257576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you for reading!","metadata":{}}]}